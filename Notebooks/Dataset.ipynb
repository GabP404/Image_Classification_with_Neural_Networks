{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "F1Pp-aCGAlGf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cb2c8e08-9d0f-4bfd-bbc7-b271f0abb313"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /gdrive\n",
            "/gdrive/My Drive/[2023-2024] AN2DL/HW1/LAST\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/gdrive')\n",
        "%cd /gdrive/My Drive/[2023-2024] AN2DL/HW1/"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Import Libraries"
      ],
      "metadata": {
        "id": "2jeRUULwAqqV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%pip install keras-cv\n",
        "%pip install seaborn\n",
        "\n",
        "#Import general libraries\n",
        "\n",
        "import numpy as np\n",
        "from numpy import load\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score, confusion_matrix\n",
        "import seaborn as sns\n",
        "from sklearn.utils import shuffle\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "\n",
        "#Import keras and tf\n",
        "\n",
        "import keras_cv\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras as tfk\n",
        "from keras import layers as tfkl\n",
        "import logging\n",
        "from tensorflow.keras.layers.experimental.preprocessing import RandomRotation\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "\n",
        "#Fix seeds for reproducibility\n",
        "seed = 42\n",
        "np.random.seed = seed\n",
        "\n",
        "#Some settings\n",
        "tf.autograph.set_verbosity(0)\n",
        "tf.get_logger().setLevel(logging.ERROR)\n",
        "tf.compat.v1.logging.set_verbosity(tf.compat.v1.logging.ERROR)\n",
        "print(tf.__version__)"
      ],
      "metadata": {
        "id": "raDZ9CmuAsKQ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1e2023a2-327d-41cd-9a69-824753e1401e"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting keras-cv\n",
            "  Downloading keras_cv-0.6.4-py3-none-any.whl (803 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m803.1/803.1 kB\u001b[0m \u001b[31m9.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from keras-cv) (23.2)\n",
            "Requirement already satisfied: absl-py in /usr/local/lib/python3.10/dist-packages (from keras-cv) (1.4.0)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.10/dist-packages (from keras-cv) (2023.6.3)\n",
            "Requirement already satisfied: tensorflow-datasets in /usr/local/lib/python3.10/dist-packages (from keras-cv) (4.9.3)\n",
            "Collecting keras-core (from keras-cv)\n",
            "  Downloading keras_core-0.1.7-py3-none-any.whl (950 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m950.8/950.8 kB\u001b[0m \u001b[31m46.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from keras-core->keras-cv) (1.23.5)\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.10/dist-packages (from keras-core->keras-cv) (13.7.0)\n",
            "Collecting namex (from keras-core->keras-cv)\n",
            "  Downloading namex-0.0.7-py3-none-any.whl (5.8 kB)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.10/dist-packages (from keras-core->keras-cv) (3.9.0)\n",
            "Requirement already satisfied: dm-tree in /usr/local/lib/python3.10/dist-packages (from keras-core->keras-cv) (0.1.8)\n",
            "Requirement already satisfied: array-record in /usr/local/lib/python3.10/dist-packages (from tensorflow-datasets->keras-cv) (0.5.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from tensorflow-datasets->keras-cv) (8.1.7)\n",
            "Requirement already satisfied: etils[enp,epath,etree]>=0.9.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow-datasets->keras-cv) (1.5.2)\n",
            "Requirement already satisfied: promise in /usr/local/lib/python3.10/dist-packages (from tensorflow-datasets->keras-cv) (2.3)\n",
            "Requirement already satisfied: protobuf>=3.20 in /usr/local/lib/python3.10/dist-packages (from tensorflow-datasets->keras-cv) (3.20.3)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from tensorflow-datasets->keras-cv) (5.9.5)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow-datasets->keras-cv) (2.31.0)\n",
            "Requirement already satisfied: tensorflow-metadata in /usr/local/lib/python3.10/dist-packages (from tensorflow-datasets->keras-cv) (1.14.0)\n",
            "Requirement already satisfied: termcolor in /usr/local/lib/python3.10/dist-packages (from tensorflow-datasets->keras-cv) (2.3.0)\n",
            "Requirement already satisfied: toml in /usr/local/lib/python3.10/dist-packages (from tensorflow-datasets->keras-cv) (0.10.2)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from tensorflow-datasets->keras-cv) (4.66.1)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.10/dist-packages (from tensorflow-datasets->keras-cv) (1.14.1)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from etils[enp,epath,etree]>=0.9.0->tensorflow-datasets->keras-cv) (2023.6.0)\n",
            "Requirement already satisfied: importlib_resources in /usr/local/lib/python3.10/dist-packages (from etils[enp,epath,etree]>=0.9.0->tensorflow-datasets->keras-cv) (6.1.1)\n",
            "Requirement already satisfied: typing_extensions in /usr/local/lib/python3.10/dist-packages (from etils[enp,epath,etree]>=0.9.0->tensorflow-datasets->keras-cv) (4.5.0)\n",
            "Requirement already satisfied: zipp in /usr/local/lib/python3.10/dist-packages (from etils[enp,epath,etree]>=0.9.0->tensorflow-datasets->keras-cv) (3.17.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->tensorflow-datasets->keras-cv) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->tensorflow-datasets->keras-cv) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->tensorflow-datasets->keras-cv) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->tensorflow-datasets->keras-cv) (2023.7.22)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from promise->tensorflow-datasets->keras-cv) (1.16.0)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich->keras-core->keras-cv) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich->keras-core->keras-cv) (2.16.1)\n",
            "Requirement already satisfied: googleapis-common-protos<2,>=1.52.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow-metadata->tensorflow-datasets->keras-cv) (1.61.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich->keras-core->keras-cv) (0.1.2)\n",
            "Installing collected packages: namex, keras-core, keras-cv\n",
            "Successfully installed keras-core-0.1.7 keras-cv-0.6.4 namex-0.0.7\n",
            "Requirement already satisfied: seaborn in /usr/local/lib/python3.10/dist-packages (0.12.2)\n",
            "Requirement already satisfied: numpy!=1.24.0,>=1.17 in /usr/local/lib/python3.10/dist-packages (from seaborn) (1.23.5)\n",
            "Requirement already satisfied: pandas>=0.25 in /usr/local/lib/python3.10/dist-packages (from seaborn) (1.5.3)\n",
            "Requirement already satisfied: matplotlib!=3.6.1,>=3.1 in /usr/local/lib/python3.10/dist-packages (from seaborn) (3.7.1)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib!=3.6.1,>=3.1->seaborn) (1.2.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib!=3.6.1,>=3.1->seaborn) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib!=3.6.1,>=3.1->seaborn) (4.44.3)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib!=3.6.1,>=3.1->seaborn) (1.4.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib!=3.6.1,>=3.1->seaborn) (23.2)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib!=3.6.1,>=3.1->seaborn) (9.4.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib!=3.6.1,>=3.1->seaborn) (3.1.1)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib!=3.6.1,>=3.1->seaborn) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=0.25->seaborn) (2023.3.post1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib!=3.6.1,>=3.1->seaborn) (1.16.0)\n",
            "Using TensorFlow backend\n",
            "2.14.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Load Dataset"
      ],
      "metadata": {
        "id": "h-rGIphsAtG4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Load the set\n",
        "dataset = load(\"public_data.npz\", allow_pickle=True)\n",
        "lst = dataset.files\n",
        "for name in lst:\n",
        "    array = dataset[name]  # Access the array by its name\n",
        "    # You can perform operations on the array here\n",
        "    print(f\"Array Name: {name}\")\n",
        "    print(\"Array Shape:\")\n",
        "    print(array.shape)\n",
        "\n",
        "#Calculate numeric labels\n",
        "unique_labels = np.unique(dataset['labels'])\n",
        "label_to_int = {label: index for index, label in enumerate(unique_labels)}\n",
        "label_to_int\n",
        "desired_output = [label_to_int[lab] for lab in dataset['labels']]\n",
        "\n",
        "LabelDataFrame = pd.DataFrame(dataset['labels'])\n",
        "intDataFrame = pd.DataFrame(desired_output)\n",
        "\n",
        "print(f\"Original labels count: {LabelDataFrame.value_counts()}\")\n",
        "print(f\"Integer labels count: {intDataFrame.value_counts()}\")\n",
        "\n",
        "images = (dataset['data'] / 255).astype(np.float32)\n",
        "width = height = dataset['data'][0].shape[0]"
      ],
      "metadata": {
        "id": "KnJpFlH1AtBi",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c9dc3449-6ebc-40df-fe02-81d4aaef6976"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Array Name: data\n",
            "Array Shape:\n",
            "(5200, 96, 96, 3)\n",
            "Array Name: labels\n",
            "Array Shape:\n",
            "(5200,)\n",
            "Original labels count: healthy      3199\n",
            "unhealthy    2001\n",
            "dtype: int64\n",
            "Integer labels count: 0    3199\n",
            "1    2001\n",
            "dtype: int64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Clean"
      ],
      "metadata": {
        "id": "lrzYk7a-A2Bf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Clean wrong images\n",
        "\n",
        "# Images from the dataset\n",
        "X = images\n",
        "\n",
        "# Create labels: 1 for 'unhealthy', 0 for 'healty'\n",
        "y = intDataFrame\n",
        "\n",
        "y = tfk.utils.to_categorical(y,len(np.unique(y)))\n",
        "\n",
        "#Cleaning this shit out\n",
        "target_value1 = images[506] #shrek\n",
        "\n",
        "target_value2 = images[529] #trololo\n",
        "\n",
        "# Create a boolean mask that checks for equality with the target image\n",
        "mask = np.all(images == target_value1, axis=(1, 2, 3))\n",
        "\n",
        "# Use the mask to filter the original array and remove matching instances\n",
        "filtered_array = images[~mask]\n",
        "\n",
        "matching_indices = np.where(mask)[0]\n",
        "filtered_labels = y[~mask]\n",
        "\n",
        "# Create a boolean mask that checks for equality with the target image\n",
        "mask2 = np.all(filtered_array == target_value2, axis=(1, 2, 3))\n",
        "\n",
        "# Use the mask to filter the original array and remove matching instances\n",
        "filtered_array = filtered_array[~mask2]\n",
        "filtered_labels = filtered_labels[~mask2]\n",
        "\n",
        "print(\"Original array shape:\", images.shape)\n",
        "print(\"Filtered array shape:\", filtered_array.shape)\n",
        "print(\"Filtered labels shape:\", filtered_labels.shape)\n",
        "\n",
        "X = filtered_array\n",
        "y = filtered_labels\n",
        "\n",
        "#End  of Cleaning this shit out"
      ],
      "metadata": {
        "id": "HoUiJeBvA3C9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1cbed0ba-0239-447a-d650-f17bae473776"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original array shape: (5200, 96, 96, 3)\n",
            "Filtered array shape: (5004, 96, 96, 3)\n",
            "Filtered labels shape: (5004, 2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Save cleaned**"
      ],
      "metadata": {
        "id": "3KxXTPrdA4F9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "np.save(\"X_cleaned.npy\", X)\n",
        "np.save(\"y_cleaned.npy\", y)"
      ],
      "metadata": {
        "id": "DKHA9IUHA5oJ"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Balance"
      ],
      "metadata": {
        "id": "VBqAEDrrA_we"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Oversample using data augmentation the minor class\n",
        "\n",
        "\n",
        "# Separate the dataset into majority and minority classes\n",
        "majority_class_indices = np.where(y[:, 0] == 1)[0]\n",
        "minority_class_indices = np.where(y[:, 1] == 1)[0]\n",
        "\n",
        "# Calculate the size of the minority class\n",
        "minority_size = len(minority_class_indices)\n",
        "\n",
        "# Calculate the size of the majority class\n",
        "majority_size = len(majority_class_indices)\n",
        "\n",
        "# Calculate the number of additional samples to generate for the minority class\n",
        "additional_minority_samples = majority_size - minority_size\n",
        "\n",
        "# Data augmentation options for the oversampled minority class\n",
        "datagen_minority = ImageDataGenerator(\n",
        "    rotation_range=75,\n",
        "    zoom_range=0.25,\n",
        "    horizontal_flip=True\n",
        ")\n",
        "\n",
        "# Apply data augmentation to the minority class to reach the desired count\n",
        "X_oversampled_minority_augmented = []\n",
        "y_oversampled_minority_augmented = []\n",
        "for index in minority_class_indices:\n",
        "    x = X[index]\n",
        "    X_oversampled_minority_augmented.append(x)  # Add the original sample\n",
        "    y_oversampled_minority_augmented.append(y[index])  # Add the original label\n",
        "\n",
        "# Randomly select and duplicate some augmented samples to reach the desired count\n",
        "while additional_minority_samples > 0:\n",
        "    index_to_duplicate = np.random.choice(minority_class_indices)\n",
        "    x = X[index_to_duplicate]\n",
        "    X_oversampled_minority_augmented.append(datagen_minority.random_transform(x))\n",
        "    y_oversampled_minority_augmented.append(y[index_to_duplicate])\n",
        "    additional_minority_samples -= 1\n",
        "\n",
        "X_oversampled_minority_augmented = np.array(X_oversampled_minority_augmented)\n",
        "y_oversampled_minority_augmented = np.array(y_oversampled_minority_augmented)\n",
        "\n",
        "# Combine the oversampled and augmented minority class with the majority class\n",
        "X_balanced = np.concatenate([X_oversampled_minority_augmented, X[majority_class_indices]])\n",
        "y_balanced = np.concatenate([y_oversampled_minority_augmented, y[majority_class_indices]])\n",
        "\n",
        "# Shuffle the balanced dataset\n",
        "X_balanced, y_balanced = shuffle(X_balanced, y_balanced, random_state=seed)\n",
        "\n",
        "X = X_balanced\n",
        "y = y_balanced"
      ],
      "metadata": {
        "id": "tOjKq_jUBCCP"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Save balanced**"
      ],
      "metadata": {
        "id": "oktklP3nBHO4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0X4WE4_JHKsd",
        "outputId": "8ab1790b-a235-457e-db6b-af24e10b6dfb"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(6202, 96, 96, 3)"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HScYFDd8HMUs",
        "outputId": "d15df0a5-6ed6-4279-c1f4-86b5283a2165"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(6202, 2)"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "np.save(\"X_balanced.npy\",X)\n",
        "np.save(\"y_balanced.npy\",y)"
      ],
      "metadata": {
        "id": "2Zz91vK_BJPJ"
      },
      "execution_count": 10,
      "outputs": []
    }
  ]
}